{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lí thuyết"
      ],
      "metadata": {
        "id": "qWYzOXFTOCTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List\n",
        "* Cấu trúc tuần tự, có thứ tự, mutable.\n",
        "* Chứa phần tử dị loại.\n",
        "* Truy cập bằng chỉ số, hỗ trợ slice và stride.\n",
        "* Toán tử in duyệt tuyến tính.\n",
        "* Ghép nối: +, mở rộng tại chỗ: extend, thêm đơn: append.\n",
        "* Unpack hỗ trợ gán nhiều biến."
      ],
      "metadata": {
        "id": "eMx_8AAIOHvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tạo list\n",
        "integer_list = [1, 2, 3]\n",
        "heterogeneous_list = [\"string\", 0.1, True]\n",
        "list_of_lists = [integer_list, heterogeneous_list, []]\n",
        "\n",
        "# độ dài, tổng\n",
        "list_length = len(integer_list)     # 3\n",
        "list_sum    = sum(integer_list)     # 6\n",
        "\n",
        "# truy cập và gán\n",
        "x = [0,1,2,3,4,5,6,7,8,9]\n",
        "zero = x[0]        # 0\n",
        "nine = x[-1]       # 9\n",
        "x[0] = -1          # [-1,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "# slice và stride\n",
        "first_three = x[:3]          # [-1,1,2]\n",
        "three_to_end = x[3:]         # [3,4,5,6,7,8,9]\n",
        "without_first_last = x[1:-1] # [1,2,3,4,5,6,7,8]\n",
        "every_third = x[::3]         # [-1,3,6,9]\n",
        "five_to_three = x[5:2:-1]    # [5,4,3]\n",
        "\n",
        "# membership\n",
        "_ = (1 in [1,2,3])  # True\n",
        "\n",
        "# ghép nối và mở rộng\n",
        "x = [1,2,3]\n",
        "x.extend([4,5,6])   # [1,2,3,4,5,6]\n",
        "y = [1,2,3] + [4,5,6]  # [1,2,3,4,5,6]\n",
        "\n",
        "# append và unpack\n",
        "x = [1,2,3]\n",
        "x.append(0)   # [1,2,3,0]\n",
        "a, b = [1, 2] # a=1, b=2\n",
        "_, b = [1, 2] # bỏ a\n"
      ],
      "metadata": {
        "id": "20iSKVdJPOE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuple\n",
        "* Giống list nhưng immutable.\n",
        "* Khai báo bằng () hoặc không cần ngoặc khi gán nhiều.\n",
        "* Dùng để trả về nhiều giá trị.\n",
        "* Hỗ trợ hoán đổi biến gọn."
      ],
      "metadata": {
        "id": "cMQwP72GPWfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2]\n",
        "my_tuple = (1, 2)\n",
        "other_tuple = 3, 4\n",
        "\n",
        "my_list[1] = 3      # OK\n",
        "try:\n",
        "    my_tuple[1] = 3 # lỗi vì immutable\n",
        "except TypeError:\n",
        "    pass\n",
        "\n",
        "def sum_and_product(x, y):\n",
        "    return (x + y), (x * y)\n",
        "\n",
        "sp = sum_and_product(2, 3)  # (5, 6)\n",
        "s, p = sum_and_product(5, 10)  # s=15, p=50\n",
        "\n",
        "x, y = 1, 2\n",
        "x, y = y, x  # hoán đổi"
      ],
      "metadata": {
        "id": "e27CYWkSPVwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dict\n",
        "* Bản đồ key -> value, tra cứu nhanh theo khóa.\n",
        "* Khởi tạo bằng {} hoặc dict().\n",
        "* Truy cập bằng [], an toàn bằng get.\n",
        "* in kiểm tra khóa.\n",
        "* Khóa phải hashable; không dùng list làm khóa."
      ],
      "metadata": {
        "id": "cb2HCt77PmEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_dict = {}\n",
        "grades = {\"Joel\": 80, \"Tim\": 95}\n",
        "\n",
        "joels_grade = grades[\"Joel\"]           # 80\n",
        "joel_has_grade = \"Joel\" in grades      # True\n",
        "kates_grade = grades.get(\"Kate\", 0)    # 0\n",
        "\n",
        "grades[\"Tim\"] = 99     # cập nhật\n",
        "grades[\"Kate\"] = 100   # thêm\n",
        "num_students = len(grades)  # 3\n",
        "\n",
        "tweet = {\n",
        "    \"user\": \"joelgrus\",\n",
        "    \"text\": \"Data Science is Awesome\",\n",
        "    \"retweet_count\": 100,\n",
        "    \"hashtags\": [\"#data\", \"#science\", \"#datascience\", \"#awesome\", \"#yolo\"],\n",
        "}\n",
        "\n",
        "tweet_keys = tweet.keys()\n",
        "tweet_values = tweet.values()\n",
        "tweet_items = tweet.items()\n",
        "\n",
        "\"user\" in tweet          # True\n",
        "\"joelgrus\" in tweet_values  # True\n"
      ],
      "metadata": {
        "id": "0YBYuHuRPsV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defaultdict\n",
        "* defaultdict(func_zero_arg) tự tạo giá trị mặc định khi thiếu khóa.\n",
        "* Giảm rẽ nhánh khi đếm hoặc gom nhóm."
      ],
      "metadata": {
        "id": "011H4fZWP5PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# đếm từ\n",
        "document = [\"a\", \"b\", \"a\"]\n",
        "word_counts = defaultdict(int)  # int() -> 0\n",
        "for w in document:\n",
        "    word_counts[w] += 1  # {'a':2, 'b':1}\n",
        "\n",
        "# gom danh sách theo khóa\n",
        "dd_list = defaultdict(list)   # list() -> []\n",
        "dd_list[2].append(1)          # {2: [1]}\n",
        "\n",
        "# lồng dict\n",
        "dd_dict = defaultdict(dict)   # dict() -> {}\n",
        "dd_dict[\"Joel\"][\"City\"] = \"Seattle\"\n",
        "\n",
        "# giá trị tùy biến\n",
        "dd_pair = defaultdict(lambda: [0, 0])\n",
        "dd_pair[2][1] = 1             # {2: [0,1]}\n"
      ],
      "metadata": {
        "id": "kO_XT50iP_ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set\n",
        "* Tập hợp phần tử phân biệt, không thứ tự, mutable.\n",
        "* Kiểm tra thành viên rất nhanh.\n",
        "* Dùng để loại trùng và các phép toán tập hợp."
      ],
      "metadata": {
        "id": "IjikbNLYQERS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tạo set\n",
        "primes_below_10 = {2, 3, 5, 7}\n",
        "s = set()\n",
        "s.add(1); s.add(2); s.add(2)  # {1,2}\n",
        "len_s = len(s)                # 2\n",
        "is_in = (2 in s)              # True\n",
        "\n",
        "# membership nhanh hơn list khi lớn\n",
        "stopwords_list = [\"a\", \"an\", \"at\", \"yet\", \"you\"]\n",
        "stopwords_set = set(stopwords_list)\n",
        "_ = (\"zip\" in stopwords_set)  # nhanh\n",
        "\n",
        "# loại trùng\n",
        "item_list = [1, 2, 3, 1, 2, 3]\n",
        "item_set = set(item_list)                 # {1,2,3}\n",
        "distinct_item_list = list(item_set)       # [1,2,3]"
      ],
      "metadata": {
        "id": "SRTXXtyAQKes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bài tập"
      ],
      "metadata": {
        "id": "3D_JxNd1OCKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JucG5bikOAfX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import argparse\n",
        "\n",
        "# ===== Tiền xử lý chung =====\n",
        "WORD_RE = re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Bóc tách thành dãy token chữ cái Unicode đã lower().\"\"\"\n",
        "    return WORD_RE.findall(text.lower())\n",
        "\n",
        "def make_ngrams(tokens, n):\n",
        "    \"\"\"Sinh n-gram bằng kỹ thuật zip trượt cửa sổ: zip(tokens, tokens[1:], ..., tokens[n-1:]).\"\"\"\n",
        "    if n <= 0 or len(tokens) < n:\n",
        "        return []\n",
        "    slices = [tokens[i:] for i in range(n)]\n",
        "    return list(zip(*slices))\n",
        "\n",
        "# ===== Bài 1: đếm n-gram (không dùng Counter) =====\n",
        "def count_ngrams_map(tokens, n):\n",
        "    \"\"\"Đếm n-gram bằng dict; khoá là tuple từ.\"\"\"\n",
        "    counts = {}\n",
        "    for gram in make_ngrams(tokens, n):\n",
        "        counts[gram] = counts.get(gram, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def most_common(counts, topk):\n",
        "    \"\"\"Sắp theo tần số giảm dần, rồi theo thứ tự từ vựng để ổn định.\"\"\"\n",
        "    return sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[:topk]\n",
        "\n",
        "def print_stats(counts, n, topk=30):\n",
        "    print(f\"\\n=== {n}-gram ===\")\n",
        "    print(f\"Số {n}-gram phân biệt: {len(counts)}\")\n",
        "    for gram, freq in most_common(counts, topk):\n",
        "        print(f\"{' '.join(gram)}\\t{freq}\")\n",
        "\n",
        "# ===== Bài 2: giao n-gram giữa 2 văn bản =====\n",
        "def overlap_counts(c1, c2):\n",
        "    \"\"\"Tiêu chí 1: kích thước giao set. Tiêu chí 2: tổng min(freq1, freq2) trên giao.\"\"\"\n",
        "    s1, s2 = set(c1.keys()), set(c2.keys())\n",
        "    unique_overlap = len(s1 & s2)\n",
        "    multiset_overlap = sum(min(c1[g], c2[g]) for g in (s1 & s2))\n",
        "    return unique_overlap, multiset_overlap\n",
        "\n",
        "def print_overlap_report(tokens1, tokens2, n, topk=20):\n",
        "    c1, c2 = count_ngrams_map(tokens1, n), count_ngrams_map(tokens2, n)\n",
        "    u_cnt, m_cnt = overlap_counts(c1, c2)\n",
        "    print(f\"\\n=== Bài 2: Trùng {n}-gram giữa hai văn bản ===\")\n",
        "    print(f\"Tiêu chí 1 (không tính trùng trong mỗi văn bản): {u_cnt}\")\n",
        "    print(f\"Tiêu chí 2 (có tính trùng trong mỗi văn bản): {m_cnt}\")\n",
        "    commons = [(g, min(c1[g], c2[g])) for g in (c1.keys() & c2.keys())]\n",
        "    commons.sort(key=lambda x: (-x[1], x[0]))\n",
        "    print(f\"Top giao (theo min tần số), tối đa {topk}:\")\n",
        "    for gram, w in commons[:topk]:\n",
        "        print(f\"{' '.join(gram)}\\t{w}\")\n",
        "\n",
        "# ===== Bài 3: tìm vị trí xuất hiện của 1 từ (không dùng bisect) =====\n",
        "# - Ranh giới câu: coi ., !, ? (kèm ngoặc/ngoặc kép kết thúc) là kết câu; lưu chỉ số kết thúc tăng dần.\n",
        "# - Tìm mọi vị trí khớp \\bword\\b (IGNORECASE).\n",
        "# - Xác định số câu bằng duyệt tuyến tính sent_ends; số dòng bằng đếm '\\n' trước pos.\n",
        "SENT_END_RE = re.compile(r'[.!?]+[\"”’)\\]]*')\n",
        "\n",
        "def build_sentence_boundaries(text):\n",
        "    ends = [m.end() for m in SENT_END_RE.finditer(text)]\n",
        "    if not ends or ends[-1] < len(text):\n",
        "        ends.append(len(text))\n",
        "    return ends  # danh sách chỉ số kết thúc câu tăng dần\n",
        "\n",
        "def sentence_number_at(pos, sent_ends):\n",
        "    \"\"\"Không dùng bisect: quét tuần tự cho tới khi pos <= end.\"\"\"\n",
        "    s_no = 1\n",
        "    for end in sent_ends:\n",
        "        if pos <= end:\n",
        "            return s_no\n",
        "        s_no += 1\n",
        "    return s_no  # fallback nếu pos > mọi end (không xảy ra vì có len(text))\n",
        "\n",
        "def line_number_at(text, pos):\n",
        "    \"\"\"Dòng đánh số từ 1.\"\"\"\n",
        "    return text.count('\\n', 0, pos) + 1\n",
        "\n",
        "def find_word_positions(text, word):\n",
        "    \"\"\"Trả về danh sách (sentence_no, line_no) cho mọi lần xuất hiện của từ.\"\"\"\n",
        "    pat = re.compile(rf'\\b{re.escape(word)}\\b', flags=re.IGNORECASE | re.UNICODE)\n",
        "    sent_ends = build_sentence_boundaries(text)\n",
        "    out = []\n",
        "    for m in pat.finditer(text):\n",
        "        pos = m.start()\n",
        "        s_no = sentence_number_at(pos, sent_ends)\n",
        "        l_no = line_number_at(text, pos)\n",
        "        out.append((s_no, l_no))\n",
        "    return out\n",
        "\n",
        "def print_positions(word, positions):\n",
        "    print(f\"\\n=== Bài 3: Vị trí từ '{word}' ===\")\n",
        "    print(f\"Số lần xuất hiện: {len(positions)}\")\n",
        "    for i, (s_no, l_no) in enumerate(positions, 1):\n",
        "        print(f\"{i}\\tCâu {s_no}\\tDòng {l_no}\")\n",
        "\n",
        "# ===== I/O =====\n",
        "def read_text(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CLI tối giản cho ba bài:\n",
        "    # - file1: bắt buộc, dùng cho Bài 1 và làm nguồn cho Bài 3.\n",
        "    # - --file2: tuỳ chọn, nếu có thì chạy Bài 2 trên 2-gram và 3-gram.\n",
        "    # - --topk: số dòng top tần số hiển thị.\n",
        "    # - --find: từ cần tìm trong file1 cho Bài 3.\n",
        "    ap = argparse.ArgumentParser(description=\"Bài 1+2+3: n-gram và tìm vị trí từ (không dùng Counter, không dùng bisect)\")\n",
        "    ap.add_argument(\"file1\", help=\"Văn bản 1 (>=1000 từ cho Bài 1)\")\n",
        "    ap.add_argument(\"--file2\", help=\"Văn bản 2 (cùng chủ đề, cho Bài 2)\")\n",
        "    ap.add_argument(\"--topk\", type=int, default=30, help=\"Số dòng top n-gram hiển thị\")\n",
        "    ap.add_argument(\"--find\", help=\"Bài 3: từ cần tìm trong file1\", default=None)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    text1 = read_text(args.file1)\n",
        "    tokens1 = tokenize(text1)\n",
        "    print(f\"Tổng số từ văn bản 1: {len(tokens1)}\")\n",
        "\n",
        "    # Bài 1\n",
        "    for n in (2, 3, 4):\n",
        "        counts = count_ngrams_map(tokens1, n)\n",
        "        print_stats(counts, n, topk=args.topk)\n",
        "\n",
        "    # Bài 2\n",
        "    if args.file2:\n",
        "        text2 = read_text(args.file2)\n",
        "        tokens2 = tokenize(text2)\n",
        "        print(f\"\\nTổng số từ văn bản 2: {len(tokens2)}\")\n",
        "        for n in (2, 3):\n",
        "            print_overlap_report(tokens1, tokens2, n, topk=min(20, args.topk))\n",
        "\n",
        "    # Bài 3\n",
        "    if args.find:\n",
        "        positions = find_word_positions(text1, args.find)\n",
        "        print_positions(args.find, positions)"
      ]
    }
  ]
}